{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7f1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librarii incarcate. Configurare GATA.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from prophet import Prophet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configurare Fisiere de intrare (generate la pasul anterior)\n",
    "INPUT_FILES = ['processed_data_House1.pkl', 'processed_data_House2.pkl']\n",
    "\n",
    "print(\"Librarii incarcate. Configurare GATA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74b969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_house(filename):\n",
    "    house_name = filename.replace('processed_data_', '').replace('.pkl', '')\n",
    "    print(f\"\\n{'='*50}\\n[START] Antrenare pentru: {house_name}\\n{'='*50}\")\n",
    "    \n",
    "    # 1. Incarcare Date\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Nu gasesc {filename}. Sari peste.\")\n",
    "        return\n",
    "\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    scaler = data['scaler']\n",
    "    df_1H = data['df_1H']\n",
    "    train_size = data['train_size']\n",
    "    WINDOW_SIZE = data['WINDOW_SIZE']\n",
    "    test_data = data['test_data'] \n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    rezultate = {}\n",
    "    timpi = {}\n",
    "\n",
    "    # --- 1. PROPHET ---\n",
    "    print(\"\\n--- 1. PROPHET ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Pregatire date Prophet\n",
    "    df_prophet = df_1H.reset_index()[['dt', 'Aggregate', 'Cluster', 'Hour_Sin', 'Hour_Cos', 'IsWeekend']]\n",
    "    df_prophet.columns = ['ds', 'y', 'Cluster', 'Hour_Sin', 'Hour_Cos', 'IsWeekend']\n",
    "    \n",
    "    df_prophet_train = df_prophet.iloc[:train_size]\n",
    "    df_prophet_test = df_prophet.iloc[train_size:]\n",
    "    \n",
    "    m = Prophet(daily_seasonality=True, weekly_seasonality=True, uncertainty_samples=0)\n",
    "    m.add_seasonality(\n",
    "    name='monthly',\n",
    "    period=30.5,\n",
    "    fourier_order=5\n",
    ")\n",
    "    m.add_regressor('Cluster')\n",
    "    m.add_regressor('Hour_Sin')\n",
    "    m.add_regressor('Hour_Cos')\n",
    "    m.add_regressor('IsWeekend')\n",
    "    \n",
    "    m.fit(df_prophet_train)\n",
    "    \n",
    "    # Predictie\n",
    "    future = m.make_future_dataframe(periods=len(df_prophet_test), freq='1H')\n",
    "    future['Cluster'] = pd.concat([df_prophet_train['Cluster'], df_prophet_test['Cluster']]).values\n",
    "    future['Hour_Sin'] = pd.concat([df_prophet_train['Hour_Sin'], df_prophet_test['Hour_Sin']]).values\n",
    "    future['Hour_Cos'] = pd.concat([df_prophet_train['Hour_Cos'], df_prophet_test['Hour_Cos']]).values\n",
    "    future['IsWeekend'] = pd.concat([df_prophet_train['IsWeekend'], df_prophet_test['IsWeekend']]).values\n",
    "    \n",
    "    forecast = m.predict(future)\n",
    "    rezultate['Prophet'] = forecast['yhat'].iloc[-len(df_prophet_test):].values\n",
    "    timpi['Prophet'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['Prophet']:.1f}s)\")\n",
    "\n",
    "    # --- 2. LSTM (Stacked) ---\n",
    "    print(\"\\n--- 2. LSTM (Stacked) ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(64, return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(LSTM(32, return_sequences=False))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(Dense(16, activation='relu'))\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(optimizer='adam', loss=Huber(delta=1.0))\n",
    "    \n",
    "    model_lstm.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1) \n",
    "    \n",
    "    pred = model_lstm.predict(X_test, verbose=0)\n",
    "    rezultate['LSTM'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['LSTM'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['LSTM']:.1f}s)\")\n",
    "\n",
    "    # --- 3. GRU ---\n",
    "    print(\"\\n--- 3. GRU ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(GRU(64, return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))\n",
    "    model_gru.add(Dropout(0.3))\n",
    "    model_gru.add(GRU(32, return_sequences=False))\n",
    "    model_gru.add(Dense(1))\n",
    "    model_gru.compile(optimizer='adam', loss=Huber(delta=1.0))\n",
    "    \n",
    "    model_gru.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1)\n",
    "    \n",
    "    pred = model_gru.predict(X_test, verbose=0)\n",
    "    rezultate['GRU'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['GRU'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['GRU']:.1f}s)\")\n",
    "\n",
    "    # --- 4. SIMPLE RNN ---\n",
    "    print(\"\\n--- 4. SIMPLE RNN ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(SimpleRNN(32, input_shape=(WINDOW_SIZE, n_features), activation='tanh'))\n",
    "    model_rnn.add(Dropout(0.2))\n",
    "    model_rnn.add(Dense(1))\n",
    "    opt = Adam(learning_rate=0.0001, clipvalue=1.0)\n",
    "    model_rnn.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    model_rnn.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1)\n",
    "    \n",
    "    pred = model_rnn.predict(X_test, verbose=0)\n",
    "    rezultate['RNN'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['RNN'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['RNN']:.1f}s)\")\n",
    "\n",
    "    # --- 6. XGBOOST & LIGHTGBM ---\n",
    "    print(\"\\n--- 6. ML Clasic (XGBoost & LightGBM) ---\")\n",
    "    # Feature Engineering Tabelar pentru ML\n",
    "    df_ml = df_1H.copy()\n",
    "    for lag in [1, 24, 168]:\n",
    "        df_ml[f'lag_{lag}'] = df_ml['Aggregate'].shift(lag)\n",
    "    \n",
    "    df_ml['rolling_mean'] = df_ml['Aggregate'].shift(1).rolling(24).mean()\n",
    "    df_ml['rolling_std'] = df_ml['Aggregate'].shift(1).rolling(24).std()\n",
    "    df_ml.dropna(inplace=True)\n",
    "    \n",
    "    feats = [c for c in df_ml.columns if 'lag' in c or 'rolling' in c or c in ['Cluster', 'Hour_Sin', 'Hour_Cos','IsWeekend', 'DayOfWeek']]\n",
    "    \n",
    "    X_ml = df_ml[feats].values\n",
    "    y_ml = df_ml['Aggregate'].values\n",
    "    \n",
    "    # Split\n",
    "    test_len = len(test_data)\n",
    "    train_len_ml = len(y_ml) - test_len\n",
    "    \n",
    "    X_train_ml, y_train_ml = X_ml[:train_len_ml], y_ml[:train_len_ml]\n",
    "    X_test_ml, y_test_ml = X_ml[train_len_ml:], y_ml[train_len_ml:]\n",
    "    \n",
    "    # XGB\n",
    "    start = time.time()\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, n_jobs=-1, random_state=42)\n",
    "    xgb_model.fit(X_train_ml, y_train_ml)\n",
    "    rezultate['XGBoost'] = xgb_model.predict(X_test_ml)\n",
    "    timpi['XGBoost'] = time.time() - start\n",
    "    print(f\" -> XGBoost Gata ({timpi['XGBoost']:.1f}s)\")\n",
    "    \n",
    "    # LGBM\n",
    "    start = time.time()\n",
    "    lgb_model = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=31, n_jobs=-1, random_state=42, verbosity=-1)\n",
    "    lgb_model.fit(X_train_ml, y_train_ml)\n",
    "    rezultate['LightGBM'] = lgb_model.predict(X_test_ml)\n",
    "    timpi['LightGBM'] = time.time() - start\n",
    "    print(f\" -> LightGBM Gata ({timpi['LightGBM']:.1f}s)\")\n",
    "\n",
    "    # --- SALVARE REZULTATE ---\n",
    "    save_name = f'results_{house_name}.pkl'\n",
    "    package = {\n",
    "        'rezultate': rezultate,\n",
    "        'timpi': timpi,\n",
    "        'y_true': test_data['Aggregate'].values, \n",
    "        'test_index': test_data.index\n",
    "    }\n",
    "    \n",
    "    with open(save_name, 'wb') as f:\n",
    "        pickle.dump(package, f)\n",
    "        \n",
    "    print(f\"\\n[SALVAT] Rezultate salvate in {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d9853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "[START] Antrenare pentru: House1\n",
      "==================================================\n",
      "\n",
      "--- 1. PROPHET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:12:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:12:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gata (2.3s)\n",
      "\n",
      "--- 2. LSTM (Stacked) ---\n",
      "Epoch 1/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.3586\n",
      "Epoch 2/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3290\n",
      "Epoch 3/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.3159\n",
      "Epoch 4/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.3084\n",
      "Epoch 5/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.3049\n",
      "Epoch 6/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3005\n",
      "Epoch 7/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2970\n",
      "Epoch 8/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2958\n",
      "Epoch 9/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2926\n",
      "Epoch 10/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.2908\n",
      "Epoch 11/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2886\n",
      "Epoch 12/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2872\n",
      "Epoch 13/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.2863\n",
      "Epoch 14/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2845\n",
      "Epoch 15/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2810\n",
      "Epoch 16/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.2826\n",
      "Epoch 17/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2798\n",
      "Epoch 18/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2757\n",
      "Epoch 19/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.2776\n",
      "Epoch 20/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.2744\n",
      " -> Gata (62.5s)\n",
      "\n",
      "--- 3. GRU ---\n",
      "Epoch 1/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - loss: 0.3379\n",
      "Epoch 2/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.3150\n",
      "Epoch 3/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.3080\n",
      "Epoch 4/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.3041\n",
      "Epoch 5/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.3000\n",
      "Epoch 6/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2971\n",
      "Epoch 7/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2938\n",
      "Epoch 8/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2917\n",
      "Epoch 9/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2902\n",
      "Epoch 10/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2863\n",
      "Epoch 11/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2855\n",
      "Epoch 12/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.2809\n",
      "Epoch 13/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2803\n",
      "Epoch 14/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2775\n",
      "Epoch 15/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2763\n",
      "Epoch 16/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2740\n",
      "Epoch 17/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2738\n",
      "Epoch 18/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2713\n",
      "Epoch 19/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2694\n",
      "Epoch 20/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.2678\n",
      " -> Gata (67.3s)\n",
      "\n",
      "--- 4. SIMPLE RNN ---\n",
      "Epoch 1/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.8172\n",
      "Epoch 2/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2048\n",
      "Epoch 3/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0405\n",
      "Epoch 4/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9270\n",
      "Epoch 5/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8922\n",
      "Epoch 6/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8618\n",
      "Epoch 7/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8139\n",
      "Epoch 8/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7914\n",
      "Epoch 9/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7434\n",
      "Epoch 10/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7414\n",
      "Epoch 11/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7653\n",
      "Epoch 12/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7119\n",
      "Epoch 13/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6957\n",
      "Epoch 14/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6877\n",
      "Epoch 15/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6800\n",
      "Epoch 16/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6561\n",
      "Epoch 17/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6431\n",
      "Epoch 18/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6268\n",
      "Epoch 19/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6299\n",
      "Epoch 20/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6512\n",
      " -> Gata (7.3s)\n",
      "\n",
      "--- 6. ML Clasic (XGBoost & LightGBM) ---\n",
      " -> XGBoost Gata (0.6s)\n",
      " -> LightGBM Gata (0.8s)\n",
      "\n",
      "[SALVAT] Rezultate salvate in results_House1.pkl\n",
      "\n",
      "==================================================\n",
      "[START] Antrenare pentru: House2\n",
      "==================================================\n",
      "\n",
      "--- 1. PROPHET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:15:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:15:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gata (3.1s)\n",
      "\n",
      "--- 2. LSTM (Stacked) ---\n",
      "Epoch 1/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 0.4507\n",
      "Epoch 2/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.4200\n",
      "Epoch 3/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.4006\n",
      "Epoch 4/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3815\n",
      "Epoch 5/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.3695\n",
      "Epoch 6/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3629\n",
      "Epoch 7/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3562\n",
      "Epoch 8/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3526\n",
      "Epoch 9/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3465\n",
      "Epoch 10/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3436\n",
      "Epoch 11/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3328\n",
      "Epoch 12/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3330\n",
      "Epoch 13/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3258\n",
      "Epoch 14/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3229\n",
      "Epoch 15/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3200\n",
      "Epoch 16/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3218\n",
      "Epoch 17/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3176\n",
      "Epoch 18/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.3088\n",
      "Epoch 19/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3142\n",
      "Epoch 20/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.3083\n",
      " -> Gata (63.5s)\n",
      "\n",
      "--- 3. GRU ---\n",
      "Epoch 1/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.4124\n",
      "Epoch 2/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3831\n",
      "Epoch 3/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3766\n",
      "Epoch 4/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3674\n",
      "Epoch 5/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3561\n",
      "Epoch 6/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3468\n",
      "Epoch 7/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.3418\n",
      "Epoch 8/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3341\n",
      "Epoch 9/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3271\n",
      "Epoch 10/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3211\n",
      "Epoch 11/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3160\n",
      "Epoch 12/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.3146\n",
      "Epoch 13/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.3111\n",
      "Epoch 14/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3059\n",
      "Epoch 15/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.3050\n",
      "Epoch 16/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.3014\n",
      "Epoch 17/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.3023\n",
      "Epoch 18/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.2950\n",
      "Epoch 19/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.2972\n",
      "Epoch 20/20\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.2934\n",
      " -> Gata (72.4s)\n",
      "\n",
      "--- 4. SIMPLE RNN ---\n",
      "Epoch 1/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.3535\n",
      "Epoch 2/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7790\n",
      "Epoch 3/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4198\n",
      "Epoch 4/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2032\n",
      "Epoch 5/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0910\n",
      "Epoch 6/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9681\n",
      "Epoch 7/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8953\n",
      "Epoch 8/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8412\n",
      "Epoch 9/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7983\n",
      "Epoch 10/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7243\n",
      "Epoch 11/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7067\n",
      "Epoch 12/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6791\n",
      "Epoch 13/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6554\n",
      "Epoch 14/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6165\n",
      "Epoch 15/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6154\n",
      "Epoch 16/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5934\n",
      "Epoch 17/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6052\n",
      "Epoch 18/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5850\n",
      "Epoch 19/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5631\n",
      "Epoch 20/20\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5556\n",
      " -> Gata (8.2s)\n",
      "\n",
      "--- 6. ML Clasic (XGBoost & LightGBM) ---\n",
      " -> XGBoost Gata (0.5s)\n",
      " -> LightGBM Gata (0.8s)\n",
      "\n",
      "[SALVAT] Rezultate salvate in results_House2.pkl\n",
      "\n",
      "==================================================\n",
      "--- \n",
      "\n",
      "[FINAL] Toate modelele pentru toate casele au fost antrenate! ---\n",
      "GATA! Fisierele .pkl sunt generate.\n",
      "Poti trece acum la urmatorul notebook: '03_Analysis_Comparison.ipynb'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Rulam totul\n",
    "for file in INPUT_FILES:\n",
    "    train_models_for_house(file)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- \\n\\n[FINAL] Toate modelele pentru toate casele au fost antrenate! ---\")\n",
    "print(\"GATA! Fisierele .pkl sunt generate.\")\n",
    "print(\"Poti trece acum la urmatorul notebook: '03_Analysis_Comparison.ipynb'\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
