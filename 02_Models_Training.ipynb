{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7f1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librarii incarcate. Configurare GATA.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from prophet import Prophet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configurare Fisiere de intrare (generate la pasul anterior)\n",
    "INPUT_FILES = ['processed_data_House1.pkl', 'processed_data_House2.pkl']\n",
    "\n",
    "print(\"Librarii incarcate. Configurare GATA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42872ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_nll(y_true, y_pred):\n",
    "    # Modelul scoate 2 valori: Mu (Prezicerea) si Sigma (Incertitudinea)\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma_raw = y_pred[:, 1]\n",
    "    \n",
    "    # Sigma trebuie sa fie pozitiv\n",
    "    sigma = tf.nn.softplus(sigma_raw) + 1e-6\n",
    "    \n",
    "    # Formula Laplace Negative Log Likelihood\n",
    "    nll = tf.math.log(2.0 * sigma) + tf.abs(y_true - mu) / sigma\n",
    "    return tf.reduce_mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74b969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_house(filename):\n",
    "    house_name = filename.replace('processed_data_', '').replace('.pkl', '')\n",
    "    print(f\"\\n{'='*50}\\n[START] Antrenare pentru: {house_name}\\n{'='*50}\")\n",
    "    \n",
    "    # 1. Incarcare Date\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Nu gasesc {filename}. Sari peste.\")\n",
    "        return\n",
    "\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    scaler = data['scaler']\n",
    "    df_1min = data['df_1min']\n",
    "    train_size = data['train_size']\n",
    "    WINDOW_SIZE = data['WINDOW_SIZE']\n",
    "    test_data = data['test_data'] \n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    rezultate = {}\n",
    "    timpi = {}\n",
    "\n",
    "    # --- 1. PROPHET ---\n",
    "    print(\"\\n--- 1. PROPHET ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Pregatire date Prophet\n",
    "    df_prophet = df_1min.reset_index()[['dt', 'Aggregate', 'Cluster', 'Hour_Sin', 'Hour_Cos', 'IsWeekend']]\n",
    "    df_prophet.columns = ['ds', 'y', 'Cluster', 'Hour_Sin', 'Hour_Cos', 'IsWeekend']\n",
    "    \n",
    "    df_prophet_train = df_prophet.iloc[:train_size]\n",
    "    df_prophet_test = df_prophet.iloc[train_size:]\n",
    "    \n",
    "    m = Prophet(daily_seasonality=True, weekly_seasonality=True, uncertainty_samples=0)\n",
    "    m.add_regressor('Cluster')\n",
    "    m.add_regressor('Hour_Sin')\n",
    "    m.add_regressor('Hour_Cos')\n",
    "    m.add_regressor('IsWeekend')\n",
    "    \n",
    "    m.fit(df_prophet_train)\n",
    "    \n",
    "    # Predictie\n",
    "    future = m.make_future_dataframe(periods=len(df_prophet_test), freq='1min')\n",
    "    future['Cluster'] = pd.concat([df_prophet_train['Cluster'], df_prophet_test['Cluster']]).values\n",
    "    future['Hour_Sin'] = pd.concat([df_prophet_train['Hour_Sin'], df_prophet_test['Hour_Sin']]).values\n",
    "    future['Hour_Cos'] = pd.concat([df_prophet_train['Hour_Cos'], df_prophet_test['Hour_Cos']]).values\n",
    "    future['IsWeekend'] = pd.concat([df_prophet_train['IsWeekend'], df_prophet_test['IsWeekend']]).values\n",
    "    \n",
    "    forecast = m.predict(future)\n",
    "    rezultate['Prophet'] = forecast['yhat'].iloc[-len(df_prophet_test):].values\n",
    "    timpi['Prophet'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['Prophet']:.1f}s)\")\n",
    "\n",
    "    # --- 2. LSTM (Stacked) ---\n",
    "    print(\"\\n--- 2. LSTM (Stacked) ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(64, return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(LSTM(32, return_sequences=False))\n",
    "    model_lstm.add(Dropout(0.3))\n",
    "    model_lstm.add(Dense(16, activation='relu'))\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(optimizer='adam', loss=Huber(delta=1.0))\n",
    "    \n",
    "    model_lstm.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1) \n",
    "    \n",
    "    pred = model_lstm.predict(X_test, verbose=0)\n",
    "    rezultate['LSTM'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['LSTM'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['LSTM']:.1f}s)\")\n",
    "\n",
    "    # --- 3. GRU ---\n",
    "    print(\"\\n--- 3. GRU ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(GRU(64, return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))\n",
    "    model_gru.add(Dropout(0.3))\n",
    "    model_gru.add(GRU(32, return_sequences=False))\n",
    "    model_gru.add(Dense(1))\n",
    "    model_gru.compile(optimizer='adam', loss=Huber(delta=1.0))\n",
    "    \n",
    "    model_gru.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "    \n",
    "    pred = model_gru.predict(X_test, verbose=0)\n",
    "    rezultate['GRU'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['GRU'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['GRU']:.1f}s)\")\n",
    "\n",
    "    # --- 4. SIMPLE RNN ---\n",
    "    print(\"\\n--- 4. SIMPLE RNN ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(SimpleRNN(32, input_shape=(WINDOW_SIZE, n_features), activation='tanh'))\n",
    "    model_rnn.add(Dropout(0.2))\n",
    "    model_rnn.add(Dense(1))\n",
    "    opt = Adam(learning_rate=0.0001, clipvalue=1.0)\n",
    "    model_rnn.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    model_rnn.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "    \n",
    "    pred = model_rnn.predict(X_test, verbose=0)\n",
    "    rezultate['RNN'] = scaler.inverse_transform(pred).flatten()\n",
    "    timpi['RNN'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['RNN']:.1f}s)\")\n",
    "\n",
    "    # --- 5. DEEPAR ---\n",
    "    print(\"\\n--- 5. DEEPAR (Probabilistic) ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    model_deepar = Sequential()\n",
    "    model_deepar.add(LSTM(64, return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))\n",
    "    model_deepar.add(Dropout(0.3))\n",
    "    model_deepar.add(LSTM(32, return_sequences=False))\n",
    "    model_deepar.add(Dropout(0.3))\n",
    "    model_deepar.add(Dense(16, activation=\"relu\"))\n",
    "    model_deepar.add(Dense(2)) # Mu si Sigma\n",
    "    model_deepar.compile(optimizer='adam', loss=laplace_nll)\n",
    "    \n",
    "    model_deepar.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "    \n",
    "    pred_params = model_deepar.predict(X_test, verbose=0)\n",
    "    # Luam doar Mu (media) pentru predictia punctuala\n",
    "    mu_pred = pred_params[:, 0]\n",
    "    rezultate['DeepAR'] = scaler.inverse_transform(mu_pred.reshape(-1, 1)).flatten()\n",
    "    timpi['DeepAR'] = time.time() - start\n",
    "    print(f\" -> Gata ({timpi['DeepAR']:.1f}s)\")\n",
    "\n",
    "    # --- 6. XGBOOST & LIGHTGBM ---\n",
    "    print(\"\\n--- 6. ML Clasic (XGBoost & LightGBM) ---\")\n",
    "    # Feature Engineering Tabelar pentru ML\n",
    "    df_ml = df_1min.copy()\n",
    "    for lag in [1, 5, 15, 60]:\n",
    "        df_ml[f'lag_{lag}'] = df_ml['Aggregate'].shift(lag)\n",
    "    \n",
    "    df_ml['rolling_mean'] = df_ml['Aggregate'].shift(1).rolling(60).mean()\n",
    "    df_ml['rolling_std'] = df_ml['Aggregate'].shift(1).rolling(60).std()\n",
    "    df_ml.dropna(inplace=True)\n",
    "    \n",
    "    feats = [c for c in df_ml.columns if 'lag' in c or 'rolling' in c or c in ['Cluster', 'Hour_Sin', 'Hour_Cos','IsWeekend', 'DayOfWeek']]\n",
    "    \n",
    "    X_ml = df_ml[feats].values\n",
    "    y_ml = df_ml['Aggregate'].values\n",
    "    \n",
    "    # Split\n",
    "    test_len = len(test_data)\n",
    "    train_len_ml = len(y_ml) - test_len\n",
    "    \n",
    "    X_train_ml, y_train_ml = X_ml[:train_len_ml], y_ml[:train_len_ml]\n",
    "    X_test_ml, y_test_ml = X_ml[train_len_ml:], y_ml[train_len_ml:]\n",
    "    \n",
    "    # XGB\n",
    "    start = time.time()\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, n_jobs=-1, random_state=42)\n",
    "    xgb_model.fit(X_train_ml, y_train_ml)\n",
    "    rezultate['XGBoost'] = xgb_model.predict(X_test_ml)\n",
    "    timpi['XGBoost'] = time.time() - start\n",
    "    print(f\" -> XGBoost Gata ({timpi['XGBoost']:.1f}s)\")\n",
    "    \n",
    "    # LGBM\n",
    "    start = time.time()\n",
    "    lgb_model = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=31, n_jobs=-1, random_state=42, verbosity=-1)\n",
    "    lgb_model.fit(X_train_ml, y_train_ml)\n",
    "    rezultate['LightGBM'] = lgb_model.predict(X_test_ml)\n",
    "    timpi['LightGBM'] = time.time() - start\n",
    "    print(f\" -> LightGBM Gata ({timpi['LightGBM']:.1f}s)\")\n",
    "\n",
    "    # --- SALVARE REZULTATE ---\n",
    "    save_name = f'results_{house_name}.pkl'\n",
    "    package = {\n",
    "        'rezultate': rezultate,\n",
    "        'timpi': timpi,\n",
    "        'y_true': test_data['Aggregate'].values, \n",
    "        'test_index': test_data.index\n",
    "    }\n",
    "    \n",
    "    with open(save_name, 'wb') as f:\n",
    "        pickle.dump(package, f)\n",
    "        \n",
    "    print(f\"\\n[SALVAT] Rezultate salvate in {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d9853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "[START] Antrenare pentru: House1\n",
      "==================================================\n",
      "\n",
      "--- 1. PROPHET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:06:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:14:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gata (488.3s)\n",
      "\n",
      "--- 2. LSTM (Stacked) ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 32ms/step - loss: 0.2154\n",
      "Epoch 2/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 33ms/step - loss: 0.1803\n",
      "Epoch 3/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 33ms/step - loss: 0.1708\n",
      "Epoch 4/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 33ms/step - loss: 0.1662\n",
      "Epoch 5/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 33ms/step - loss: 0.1612\n",
      "Epoch 6/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 33ms/step - loss: 0.1592\n",
      "Epoch 7/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 33ms/step - loss: 0.1565\n",
      "Epoch 8/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 33ms/step - loss: 0.1547\n",
      "Epoch 9/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 32ms/step - loss: 0.1521\n",
      "Epoch 10/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 32ms/step - loss: 0.1512\n",
      " -> Gata (3818.2s)\n",
      "\n",
      "--- 3. GRU ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 37ms/step - loss: 0.2065\n",
      "Epoch 2/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 37ms/step - loss: 0.1695\n",
      "Epoch 3/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 37ms/step - loss: 0.1625\n",
      "Epoch 4/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 37ms/step - loss: 0.1587\n",
      "Epoch 5/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 37ms/step - loss: 0.1558\n",
      "Epoch 6/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 37ms/step - loss: 0.1536\n",
      "Epoch 7/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 38ms/step - loss: 0.1527\n",
      "Epoch 8/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 37ms/step - loss: 0.1507\n",
      "Epoch 9/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 37ms/step - loss: 0.1502\n",
      "Epoch 10/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 37ms/step - loss: 0.1492\n",
      " -> Gata (4321.7s)\n",
      "\n",
      "--- 4. SIMPLE RNN ---\n",
      "Epoch 1/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 2.7295\n",
      "Epoch 2/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 2.2089\n",
      "Epoch 3/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 6ms/step - loss: 2.0120\n",
      "Epoch 4/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 6ms/step - loss: 1.9055\n",
      "Epoch 5/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 6ms/step - loss: 1.8328\n",
      "Epoch 6/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 1.7949\n",
      "Epoch 7/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 1.7696\n",
      "Epoch 8/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 1.7485\n",
      "Epoch 9/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 1.7378\n",
      "Epoch 10/10\n",
      "\u001b[1m5751/5751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 6ms/step - loss: 1.7312\n",
      " -> Gata (387.1s)\n",
      "\n",
      "--- 5. DEEPAR (Probabilistic) ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 32ms/step - loss: -0.2899\n",
      "Epoch 2/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.6255\n",
      "Epoch 3/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.7078\n",
      "Epoch 4/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.7838\n",
      "Epoch 5/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 31ms/step - loss: -0.8516\n",
      "Epoch 6/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.9028\n",
      "Epoch 7/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 31ms/step - loss: -0.9219\n",
      "Epoch 8/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.9432\n",
      "Epoch 9/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.9551\n",
      "Epoch 10/10\n",
      "\u001b[1m11501/11501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 31ms/step - loss: -0.9714\n",
      " -> Gata (3648.6s)\n",
      "\n",
      "--- 6. ML Clasic (XGBoost & LightGBM) ---\n",
      " -> XGBoost Gata (8.6s)\n",
      " -> LightGBM Gata (4.3s)\n",
      "\n",
      "[SALVAT] Rezultate salvate in results_House1.pkl\n",
      "\n",
      "==================================================\n",
      "[START] Antrenare pentru: House2\n",
      "==================================================\n",
      "\n",
      "--- 1. PROPHET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:38:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:42:24 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gata (305.3s)\n",
      "\n",
      "--- 2. LSTM (Stacked) ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 32ms/step - loss: 0.3197\n",
      "Epoch 2/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 32ms/step - loss: 0.2791\n",
      "Epoch 3/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 32ms/step - loss: 0.2672\n",
      "Epoch 4/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 32ms/step - loss: 0.2576\n",
      "Epoch 5/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 32ms/step - loss: 0.2521\n",
      "Epoch 6/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 32ms/step - loss: 0.2504\n",
      "Epoch 7/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 32ms/step - loss: 0.2467\n",
      "Epoch 8/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 32ms/step - loss: 0.2440\n",
      "Epoch 9/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 32ms/step - loss: 0.2421\n",
      "Epoch 10/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 32ms/step - loss: 0.2415\n",
      " -> Gata (3617.2s)\n",
      "\n",
      "--- 3. GRU ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 37ms/step - loss: 0.3220\n",
      "Epoch 2/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 36ms/step - loss: 0.2438\n",
      "Epoch 3/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 37ms/step - loss: 0.2387\n",
      "Epoch 4/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 37ms/step - loss: 0.2349\n",
      "Epoch 5/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 38ms/step - loss: 0.2316\n",
      "Epoch 6/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 37ms/step - loss: 0.2306\n",
      "Epoch 7/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 37ms/step - loss: 0.2298\n",
      "Epoch 8/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 37ms/step - loss: 0.2283\n",
      "Epoch 9/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 36ms/step - loss: 0.2282\n",
      "Epoch 10/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 37ms/step - loss: 0.2270\n",
      " -> Gata (4142.7s)\n",
      "\n",
      "--- 4. SIMPLE RNN ---\n",
      "Epoch 1/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 6ms/step - loss: 12.0689\n",
      "Epoch 2/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 8.0262\n",
      "Epoch 3/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 6.4759\n",
      "Epoch 4/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 5.6051\n",
      "Epoch 5/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 6ms/step - loss: 5.1859\n",
      "Epoch 6/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 5.0089\n",
      "Epoch 7/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 4.8960\n",
      "Epoch 8/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 4.8558\n",
      "Epoch 9/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 4.8287\n",
      "Epoch 10/10\n",
      "\u001b[1m5557/5557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6ms/step - loss: 4.8302\n",
      " -> Gata (338.7s)\n",
      "\n",
      "--- 5. DEEPAR (Probabilistic) ---\n",
      "Epoch 1/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 32ms/step - loss: -0.3897\n",
      "Epoch 2/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.0196\n",
      "Epoch 3/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.1782\n",
      "Epoch 4/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.1696\n",
      "Epoch 5/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.1438\n",
      "Epoch 6/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.2432\n",
      "Epoch 7/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.1872\n",
      "Epoch 8/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 32ms/step - loss: -1.2591\n",
      "Epoch 9/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.2761\n",
      "Epoch 10/10\n",
      "\u001b[1m11113/11113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - loss: -1.3799\n",
      " -> Gata (3578.1s)\n",
      "\n",
      "--- 6. ML Clasic (XGBoost & LightGBM) ---\n",
      " -> XGBoost Gata (6.1s)\n",
      " -> LightGBM Gata (4.1s)\n",
      "\n",
      "[SALVAT] Rezultate salvate in results_House2.pkl\n",
      "\n",
      "==================================================\n",
      "--- \n",
      "\n",
      "[FINAL] Toate modelele pentru toate casele au fost antrenate! ---\n",
      "GATA! Fisierele .pkl sunt generate.\n",
      "Poti trece acum la urmatorul notebook: '03_Analysis_Comparison.ipynb'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Rulam totul\n",
    "for file in INPUT_FILES:\n",
    "    train_models_for_house(file)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- \\n\\n[FINAL] Toate modelele pentru toate casele au fost antrenate! ---\")\n",
    "print(\"GATA! Fisierele .pkl sunt generate.\")\n",
    "print(\"Poti trece acum la urmatorul notebook: '03_Analysis_Comparison.ipynb'\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
