{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7d6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURATIE\n",
    "HOUSES = [\n",
    "    {'name': 'House1', 'file': 'CLEAN_House1.csv'},\n",
    "    {'name': 'House2', 'file': 'CLEAN_House2.csv'} \n",
    "]\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "TRAIN_SPLIT = 0.8\n",
    "SAMPLE_SIZE = 50000 # Pentru optimizarea KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b37277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTIE 1: INCARCARE SI FEATURES ---\n",
    "def process_dataframe(filename):\n",
    "    print(f\"\\n[INFO] Procesare fisier: {filename}...\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['dt'] = pd.to_datetime(df['Time'])\n",
    "    df.set_index('dt', inplace=True)\n",
    "    df = df[['Aggregate']]\n",
    "    df['Aggregate'] = pd.to_numeric(df['Aggregate'], errors='coerce')\n",
    "    \n",
    "    # Resampling\n",
    "    df_1min = df.resample('1min').mean().fillna(method='ffill')\n",
    "    \n",
    "    # Features\n",
    "    df_1min['Hour'] = df_1min.index.hour\n",
    "    df_1min['DayOfWeek'] = df_1min.index.dayofweek\n",
    "    df_1min['Hour_Sin'] = np.sin(2 * np.pi * df_1min['Hour'] / 24.0)\n",
    "    df_1min['Hour_Cos'] = np.cos(2 * np.pi * df_1min['Hour'] / 24.0)\n",
    "    df_1min['IsWeekend'] = df_1min['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_1min['DayOfWeek_Scaled'] = df_1min['DayOfWeek'] / 6.0\n",
    "    \n",
    "    return df_1min\n",
    "\n",
    "# --- FUNCTIE 2: CLUSTERING ---\n",
    "def apply_clustering(df):\n",
    "    print(\"[INFO] Optimizare si aplicare KMeans...\")\n",
    "    # Esantion pentru viteza\n",
    "    data_sample = df[['Aggregate']].sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).values\n",
    "    \n",
    "    # Cautam cel mai bun k (simplificat pentru automatizare)\n",
    "    range_n_clusters = [2, 3, 4, 5]\n",
    "    scores = []\n",
    "    for k in range_n_clusters:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = km.fit_predict(data_sample)\n",
    "        scores.append(silhouette_score(data_sample, labels))\n",
    "    \n",
    "    best_k = range_n_clusters[np.argmax(scores)]\n",
    "    print(f\" -> Cel mai bun k detectat: {best_k}\")\n",
    "    \n",
    "    # Aplicare finala\n",
    "    kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    df['Cluster'] = kmeans_final.fit_predict(df[['Aggregate']].values)\n",
    "    \n",
    "    # Ordonare clustere (0 = Low, 1 = High logic)\n",
    "    centers = kmeans_final.cluster_centers_.flatten()\n",
    "    if len(centers) == 2 and centers[0] > centers[1]:\n",
    "        df['Cluster'] = 1 - df['Cluster'] # Inversare\n",
    "        \n",
    "    return df, kmeans_final\n",
    "\n",
    "# --- FUNCTIE 3: SECVENTE SI SCALARE ---\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i-window_size:i])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def prepare_and_save(df, kmeans_model, house_name):\n",
    "    print(f\"[INFO] Generare secvente pentru {house_name}...\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    df['Aggregate_Scaled'] = scaler.fit_transform(df[['Aggregate']])\n",
    "    \n",
    "    features_cols = ['Aggregate_Scaled', 'Hour_Sin', 'Hour_Cos', 'DayOfWeek_Scaled', 'IsWeekend', 'Cluster']\n",
    "    dataset = df[features_cols].values\n",
    "    \n",
    "    train_len = int(len(dataset) * TRAIN_SPLIT)\n",
    "    train_data = dataset[:train_len]\n",
    "    test_data_values = dataset[train_len:]\n",
    "    \n",
    "    X_train, y_train = create_sequences(train_data, WINDOW_SIZE)\n",
    "    test_inputs = np.concatenate((train_data[-WINDOW_SIZE:], test_data_values))\n",
    "    X_test, y_test = create_sequences(test_inputs, WINDOW_SIZE)\n",
    "    \n",
    "    # SALVARE FISER\n",
    "    save_filename = f'processed_data_{house_name}.pkl'\n",
    "    \n",
    "    data_package = {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'scaler': scaler, 'kmeans': kmeans_model,\n",
    "        'test_data': df.iloc[train_len:],\n",
    "        'df_1min': df,\n",
    "        'WINDOW_SIZE': WINDOW_SIZE,\n",
    "        'train_size': train_len \n",
    "    }\n",
    "    \n",
    "    with open(save_filename, 'wb') as f:\n",
    "        pickle.dump(data_package, f)\n",
    "        \n",
    "    print(f\"[SUCCESS] Salvat: {save_filename} | Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd1c34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "START PROCESARE: House1\n",
      "========================================\n",
      "\n",
      "[INFO] Procesare fisier: CLEAN_House1.csv...\n",
      "[INFO] Optimizare si aplicare KMeans...\n",
      " -> Cel mai bun k detectat: 2\n",
      "[INFO] Generare secvente pentru House1...\n",
      "[SUCCESS] Salvat: processed_data_House1.pkl | Train: (736012, 60, 6), Test: (184019, 60, 6)\n",
      "\n",
      "========================================\n",
      "START PROCESARE: House2\n",
      "========================================\n",
      "\n",
      "[INFO] Procesare fisier: CLEAN_House2.csv...\n",
      "[INFO] Optimizare si aplicare KMeans...\n",
      " -> Cel mai bun k detectat: 2\n",
      "[INFO] Generare secvente pentru House2...\n",
      "[SUCCESS] Salvat: processed_data_House2.pkl | Train: (711202, 60, 6), Test: (177816, 60, 6)\n",
      "\n",
      "==================================================\n",
      "--- TOATE CASELE AU FOST PROCESATE ---\n",
      "GATA! Fisierele .pkl sunt generate.\n",
      "Poti trece acum la urmatorul notebook: '02_Models_Training.ipynb'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Iteram prin fiecare casa\n",
    "for house in HOUSES:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"START PROCESARE: {house['name']}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Incarcare si Features\n",
    "        df = process_dataframe(house['file'])\n",
    "        \n",
    "        # 2. Clustering\n",
    "        df, kmeans_model = apply_clustering(df)\n",
    "        \n",
    "        # 3. Salvare (Sequences)\n",
    "        prepare_and_save(df, kmeans_model, house['name'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[EROARE] Problema la {house['name']}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- TOATE CASELE AU FOST PROCESATE ---\")\n",
    "print(\"GATA! Fisierele .pkl sunt generate.\")\n",
    "print(\"Poti trece acum la urmatorul notebook: '02_Models_Training.ipynb'\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mihai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
