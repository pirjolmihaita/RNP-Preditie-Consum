{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b7d6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURATIE\n",
    "HOUSES = [\n",
    "    {'name': 'House1', 'file': 'CLEAN_House1.csv'},\n",
    "    {'name': 'House2', 'file': 'CLEAN_House2.csv'} \n",
    "]\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "TRAIN_SPLIT = 0.8\n",
    "SAMPLE_SIZE = 50000 # Pentru optimizarea KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b37277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTIE 1: INCARCARE SI FEATURES ---\n",
    "def process_dataframe(filename):\n",
    "    print(f\"\\n[INFO] Procesare fisier: {filename}...\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['dt'] = pd.to_datetime(df['Time'])\n",
    "    df.set_index('dt', inplace=True)\n",
    "    df = df[['Aggregate']]\n",
    "    df['Aggregate'] = pd.to_numeric(df['Aggregate'], errors='coerce')\n",
    "    \n",
    "    # Resampling\n",
    "    df_1min = df.resample('1min').mean().fillna(method='ffill')\n",
    "    \n",
    "    # Features\n",
    "    df_1min['Hour'] = df_1min.index.hour\n",
    "    df_1min['DayOfWeek'] = df_1min.index.dayofweek\n",
    "    df_1min['Hour_Sin'] = np.sin(2 * np.pi * df_1min['Hour'] / 24.0)\n",
    "    df_1min['Hour_Cos'] = np.cos(2 * np.pi * df_1min['Hour'] / 24.0)\n",
    "    df_1min['IsWeekend'] = df_1min['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_1min['DayOfWeek_Scaled'] = df_1min['DayOfWeek'] / 6.0\n",
    "    \n",
    "    return df_1min\n",
    "\n",
    "# --- FUNCTIE 2: CLUSTERING ---\n",
    "def apply_clustering(df, train_len):\n",
    "    print(\"[INFO] Optimizare si aplicare KMeans (fit pe train, predict pe test)...\")\n",
    "\n",
    "    # Split pe df (doar pentru antrenarea KMeans fara leakage)\n",
    "    df_train = df.iloc[:train_len].copy()\n",
    "    df_test  = df.iloc[train_len:].copy()\n",
    "\n",
    "    # Esantion doar din train pentru alegerea lui k\n",
    "    data_sample = df_train[['Aggregate']].sample(\n",
    "        n=min(SAMPLE_SIZE, len(df_train)),\n",
    "        random_state=42\n",
    "    ).values\n",
    "\n",
    "    range_n_clusters = [2, 3, 4, 5]\n",
    "    scores = []\n",
    "    for k in range_n_clusters:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = km.fit_predict(data_sample)\n",
    "        scores.append(silhouette_score(data_sample, labels))\n",
    "\n",
    "    best_k = range_n_clusters[np.argmax(scores)]\n",
    "    print(f\" -> Cel mai bun k detectat (train): {best_k}\")\n",
    "\n",
    "    # Fit pe train, predict pe test\n",
    "    kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    df_train['Cluster'] = kmeans_final.fit_predict(df_train[['Aggregate']].values)\n",
    "    df_test['Cluster']  = kmeans_final.predict(df_test[['Aggregate']].values)\n",
    "\n",
    "    # Ordonare pentru cazul k=2 (0 = low, 1 = high)\n",
    "    centers = kmeans_final.cluster_centers_.flatten()\n",
    "    if len(centers) == 2 and centers[0] > centers[1]:\n",
    "        df_train['Cluster'] = 1 - df_train['Cluster']\n",
    "        df_test['Cluster']  = 1 - df_test['Cluster']\n",
    "\n",
    "    # Recombinare, cu aceleasi randuri ca df initial\n",
    "    df_out = pd.concat([df_train, df_test], axis=0)\n",
    "    return df_out, kmeans_final\n",
    "\n",
    "\n",
    "# --- FUNCTIE 3: SECVENTE ---\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i-window_size:i])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def prepare_and_save(df, kmeans_model, house_name):\n",
    "    print(f\"[INFO] Generare secvente pentru {house_name}...\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. Selectarea feature-urilor brute (fara scalare initiala)\n",
    "    # ============================================================\n",
    "    features_cols_raw = [\n",
    "        'Aggregate',\n",
    "        'Hour_Sin',\n",
    "        'Hour_Cos',\n",
    "        'DayOfWeek_Scaled',\n",
    "        'IsWeekend',\n",
    "        'Cluster'\n",
    "    ]\n",
    "\n",
    "    dataset_raw = df[features_cols_raw].values\n",
    "\n",
    "    # ============================================================\n",
    "    # 2. Split temporal train / test\n",
    "    # ============================================================\n",
    "    train_len = int(len(dataset_raw) * TRAIN_SPLIT)\n",
    "\n",
    "    train_raw = dataset_raw[:train_len]\n",
    "    test_raw  = dataset_raw[train_len:]\n",
    "\n",
    "    # ============================================================\n",
    "    # 3. Scalare DOAR pe setul de antrenare (evitare data leakage)\n",
    "    # ============================================================\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Coloana 0 = Aggregate\n",
    "    train_agg_scaled = scaler.fit_transform(train_raw[:, [0]])\n",
    "    test_agg_scaled  = scaler.transform(test_raw[:, [0]])\n",
    "\n",
    "    # ============================================================\n",
    "    # 4. Reconstruirea dataset-ului final scalat\n",
    "    #    Aggregate_Scaled + celelalte feature-uri nemodificate\n",
    "    # ============================================================\n",
    "    train_data = np.concatenate([train_agg_scaled, train_raw[:, 1:]], axis=1)\n",
    "    test_data_values = np.concatenate([test_agg_scaled, test_raw[:, 1:]], axis=1)\n",
    "\n",
    "    # ============================================================\n",
    "    # 5. Crearea secventelor pentru antrenare\n",
    "    # ============================================================\n",
    "    X_train, y_train = create_sequences(train_data, WINDOW_SIZE)\n",
    "\n",
    "    # ============================================================\n",
    "    # 6. Crearea secventelor pentru test\n",
    "    #    Se concateneaza ultimele WINDOW_SIZE valori din train\n",
    "    #    pentru a pastra contextul temporal\n",
    "    # ============================================================\n",
    "    test_inputs = np.concatenate((train_data[-WINDOW_SIZE:], test_data_values))\n",
    "    X_test, y_test = create_sequences(test_inputs, WINDOW_SIZE)\n",
    "\n",
    "    # ============================================================\n",
    "    # 7. Salvare fisier procesat\n",
    "    # ============================================================\n",
    "    save_filename = f'processed_data_{house_name}.pkl'\n",
    "\n",
    "    data_package = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'kmeans': kmeans_model,\n",
    "        'test_data': df.iloc[train_len:],   # datele brute din test (pentru analiza)\n",
    "        'df_1min': df,                      # dataframe-ul complet procesat\n",
    "        'WINDOW_SIZE': WINDOW_SIZE,\n",
    "        'train_size': train_len\n",
    "    }\n",
    "        \n",
    "    with open(save_filename, 'wb') as f:\n",
    "            pickle.dump(data_package, f)\n",
    "            \n",
    "    print(f\"[SUCCESS] Salvat: {save_filename} | Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd1c34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "START PROCESARE: House1\n",
      "========================================\n",
      "\n",
      "[INFO] Procesare fisier: CLEAN_House1.csv...\n",
      "[INFO] Optimizare si aplicare KMeans (fit pe train, predict pe test)...\n",
      " -> Cel mai bun k detectat (train): 2\n",
      "[INFO] Generare secvente pentru House1...\n",
      "[SUCCESS] Salvat: processed_data_House1.pkl | Train: (736012, 60, 6), Test: (184019, 60, 6)\n",
      "\n",
      "========================================\n",
      "START PROCESARE: House2\n",
      "========================================\n",
      "\n",
      "[INFO] Procesare fisier: CLEAN_House2.csv...\n",
      "[INFO] Optimizare si aplicare KMeans (fit pe train, predict pe test)...\n",
      " -> Cel mai bun k detectat (train): 2\n",
      "[INFO] Generare secvente pentru House2...\n",
      "[SUCCESS] Salvat: processed_data_House2.pkl | Train: (711202, 60, 6), Test: (177816, 60, 6)\n",
      "\n",
      "==================================================\n",
      "--- TOATE CASELE AU FOST PROCESATE ---\n",
      "GATA! Fisierele .pkl sunt generate.\n",
      "Poti trece acum la urmatorul notebook: '02_Models_Training.ipynb'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Iteram prin fiecare casa\n",
    "for house in HOUSES:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"START PROCESARE: {house['name']}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Incarcare si Features\n",
    "        df = process_dataframe(house['file'])\n",
    "        \n",
    "        # 2. Clustering\n",
    "        train_len = int(len(df) * TRAIN_SPLIT)\n",
    "        df, kmeans_model = apply_clustering(df, train_len)\n",
    "        \n",
    "        # 3. Salvare (Sequences)\n",
    "        prepare_and_save(df, kmeans_model, house['name'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[EROARE] Problema la {house['name']}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- TOATE CASELE AU FOST PROCESATE ---\")\n",
    "print(\"GATA! Fisierele .pkl sunt generate.\")\n",
    "print(\"Poti trece acum la urmatorul notebook: '02_Models_Training.ipynb'\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decdcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
